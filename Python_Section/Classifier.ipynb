{"cells":[{"cell_type":"markdown","metadata":{"id":"MaEHh974EVSI"},"source":["# GPU Configuration"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1076,"status":"ok","timestamp":1677306752782,"user":{"displayName":"Amir Hamidi","userId":"14224589576575881828"},"user_tz":-210},"id":"dY99c9wUEYtW","outputId":"f0b6d81d-97fc-4424-e069-1cd22032c2a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Feb 25 06:32:42 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P0    28W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"e10-bYLj3GRh"},"source":["# Importing Packages"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"eE50JodJlWsV","executionInfo":{"status":"ok","timestamp":1677306784986,"user_tz":-210,"elapsed":6143,"user":{"displayName":"Amir Hamidi","userId":"14224589576575881828"}}},"outputs":[],"source":["# Common Imports\n","import os\n","import random\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from time import time\n","from google.colab import drive\n","from matplotlib.image import imread\n","from sklearn.model_selection import KFold\n","\n","# PyTorch Imports\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","\n","from torchsummary import summary as Summary\n","from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler"]},{"cell_type":"markdown","metadata":{"id":"VquRrPAuzRlP"},"source":["# Classes"]},{"cell_type":"markdown","source":["## Dataset Class"],"metadata":{"id":"cBnMwcfBi8ab"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"sgZ5iYDxC68z","executionInfo":{"status":"ok","timestamp":1677306786762,"user_tz":-210,"elapsed":3,"user":{"displayName":"Amir Hamidi","userId":"14224589576575881828"}}},"outputs":[],"source":["class Medical_Dataset(Dataset):\n","\n","    def __init__(self, root_folder, class_folders=None, XTransformer=None, YTransformer=None, \n","                 Validation_Type=\"Split\", Train_Dataset=True, Validation_Subjects=None):\n","        \n","        self.XTransformer = XTransformer\n","        self.YTransformer = YTransformer\n","        self.Dataset_Size = 0\n","         \n","        if not class_folders:\n","    \n","            class_folders = os.listdir(root_folder)\n","            class_folders.sort()\n","\n","        #################### Split and KFold Validation ####################\n","        if Validation_Type == \"Split\" or Validation_Type == \"KFold\" :\n","            \n","            for class_folder in class_folders:\n","\n","                class_folder_path = os.path.join(root_folder, class_folder)\n","                self.Dataset_Size += len(os.listdir(class_folder_path))\n","\n","            Total_Images = np.empty(0)\n","            Total_Labels = torch.zeros(self.Dataset_Size, dtype=torch.int64)\n","            dataset_index = 0\n","\n","            # loop through each folder\n","            for Target, class_folder in enumerate(class_folders):\n","                \n","                class_folder_path = os.path.join(root_folder, class_folder)\n","                images_name = os.listdir(class_folder_path)\n","                images_name.sort()\n","\n","                # loop through each sample\n","                for image_name in images_name:\n","                    \n","                    Total_Images = np.append(Total_Images, os.path.join(class_folder_path, image_name))\n","                    Total_Labels[dataset_index] = Target\n","                    dataset_index += 1\n","\n","            permutation = np.random.permutation(np.arange(len(Total_Images)))\n","            \n","            self.images = Total_Images\n","            self.labels = Total_Labels\n","\n","        #################### LOO Validation ####################\n","\n","        elif Validation_Type == \"LOO\":\n","\n","            if Train_Dataset:\n","\n","                for class_folder in class_folders:\n","                    \n","                    class_validation_subjects = Validation_Subjects[class_folder]\n","                    class_folder_path = os.path.join(root_folder, class_folder)\n","                    subject_folders = os.listdir(class_folder_path)\n","                    subject_folders.sort()\n","\n","                    for class_validation_subject in class_validation_subjects:\n","                        \n","                        if class_validation_subject[-1] != \"*\":\n","\n","                            subject_folders.remove(class_validation_subject)\n","\n","\n","                    for subject_folder in subject_folders:\n","\n","                        subject_folder_path = os.path.join(class_folder_path, subject_folder)\n","                        self.Dataset_Size += len(os.listdir(subject_folder_path))\n","\n","                # define each image path and label\n","                Total_Images = np.empty(0)\n","                Total_Labels = torch.zeros(self.Dataset_Size, dtype=torch.int64)\n","                dataset_index = 0\n","                \n","                # looping through class folders\n","                for Target, class_folder in enumerate(class_folders):\n","                    \n","                    class_validation_subjects = Validation_Subjects[class_folder]\n","                    class_folder_path = os.path.join(root_folder, class_folder)\n","\n","                    subject_folders = os.listdir(class_folder_path)\n","                    subject_folders.sort()\n","\n","                    for class_validation_subject in class_validation_subjects:\n","                        \n","                        if class_validation_subject[-1] != \"*\":\n","\n","                            subject_folders.remove(class_validation_subject)\n","                    \n","\n","                    # looping through subjects\n","                    for subject_folder in subject_folders:\n","\n","                        subject_folder_path = os.path.join(class_folder_path, subject_folder)\n","                        images_name = os.listdir(subject_folder_path)\n","                        images_name.sort()\n","\n","                        # looping through images\n","                        for image_name in images_name:\n","                            # print(dataset_index)\n","                            Total_Images = np.append(Total_Images, os.path.join(subject_folder_path, image_name))\n","                            Total_Labels[dataset_index] = Target\n","                            dataset_index += 1\n","\n","            elif not Train_Dataset:\n","                \n","                # looping through class folders\n","                for class_folder in class_folders:\n","                    \n","                    class_validation_Subjects = Validation_Subjects[class_folder]\n","                    class_folder_path = os.path.join(root_folder, class_folder)\n","\n","                    subject_folders = class_validation_Subjects                      \n","                    subject_folders.sort()\n","\n","                    for subject_folder in subject_folders:\n","                        \n","                        if subject_folder[-1] == \"*\":\n","\n","                            subject_folder = subject_folder[0:-1]\n","\n","                        subject_folder_path = os.path.join(class_folder_path, subject_folder)\n","                        self.Dataset_Size += len(os.listdir(subject_folder_path))\n","\n","                Total_Images = np.empty(0)\n","                Total_Labels = torch.zeros(self.Dataset_Size, dtype=torch.int64)\n","                dataset_index = 0\n","\n","                for Target, class_folder in enumerate(class_folders):\n","\n","                    class_validation_subjects = Validation_Subjects[class_folder]\n","                    class_folder_path = os.path.join(root_folder, class_folder)\n","\n","                    subject_folders = class_validation_subjects\n","                    subject_folders.sort()\n","\n","                    for subject_folder in subject_folders:\n","                        \n","                        if subject_folder[-1] == \"*\":\n","\n","                            subject_folder = subject_folder[0:-1]\n","\n","                        subject_folder_path = os.path.join(class_folder_path, subject_folder)\n","                        images_name = os.listdir(subject_folder_path)\n","                        images_name.sort()\n","\n","                        for image_name in images_name:\n","\n","                            Total_Images = np.append(Total_Images, os.path.join(subject_folder_path, image_name))\n","                            Total_Labels[dataset_index] = Target\n","                            dataset_index += 1\n","            \n","            self.images = Total_Images\n","            self.labels = Total_Labels\n","\n","    def __len__(self):\n","\n","        return self.Dataset_Size\n","\n","    def __getitem__(self, index):\n","\n","        # Retriving Indexed Image\n","        image_path = self.images[index]\n","        image = imread(image_path)\n","\n","        if self.XTransformer:\n","            \n","            image = self.XTransformer(image)\n","\n","        # Retriving Indexed Label\n","        label = self.labels[index]\n","\n","        if self.YTransformer:\n","            \n","            label = self.YTransformer(label)\n","        \n","        return image, label"]},{"cell_type":"markdown","source":["## Network Class"],"metadata":{"id":"diJqTM9VjBJS"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"OtkQ2_P_5sY8","executionInfo":{"status":"ok","timestamp":1677306787306,"user_tz":-210,"elapsed":5,"user":{"displayName":"Amir Hamidi","userId":"14224589576575881828"}}},"outputs":[],"source":["class CNN_Model(nn.Module):\n","     \n","    def __init__(self, Input_Size, Number_of_Classes=2):\n","        \n","        super().__init__()\n","        Input_Channels = Input_Size[0]\n","        Temp_X = torch.reshape(torch.zeros(Input_Size), (1, *Input_Size))\n","\n","        # Convolutional Layers\n","        self.Convolutional_Network = nn.Sequential(\n","            \n","            # First Layer\n","            nn.Conv2d(Input_Channels, 8, 3, 1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, 2),\n","\n","            # Second Layer\n","            nn.Conv2d(8, 16, 3, 1),\n","            nn.ReLU(inplace=True),\n","            # nn.Dropout(1),\n","\n","            # Third Layer\n","            nn.Conv2d(16, 32, 3, 2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, 2),\n","\n","            # Fourth Layer\n","            nn.Conv2d(32, 96, 3, 2),\n","            nn.ReLU(inplace=True),\n","            nn.Flatten()\n","        )\n","\n","        First_Linear_Layer_Size = self.Convolutional_Network(Temp_X).shape[1]\n","\n","        # Linear Layers\n","        self.Linear_Network = nn.Sequential(\n","\n","            # Fifth Layer\n","            nn.Linear(First_Linear_Layer_Size, 50),\n","            nn.ReLU(inplace=True),\n","\n","            # Sixth Layer\n","            nn.Linear(50, 32),\n","            nn.ReLU(inplace=True),\n","\n","            # Seventh Layer\n","            nn.Linear(32, Number_of_Classes),\n","            nn.Softmax(dim=1),\n","        )\n","\n","    def forward(self, x):\n","\n","        return self.Linear_Network(self.Convolutional_Network(x))"]},{"cell_type":"markdown","source":["## Auxiliary Class"],"metadata":{"id":"05P0FnxkjVaj"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"MbPZkmc_Upg-","executionInfo":{"status":"ok","timestamp":1677306787306,"user_tz":-210,"elapsed":4,"user":{"displayName":"Amir Hamidi","userId":"14224589576575881828"}}},"outputs":[],"source":["class ToNumpy():\n","\n","    def __call__(self, input):\n","\n","        return np.array(input)"]},{"cell_type":"markdown","metadata":{"id":"i7WaIed6T1jg"},"source":["#Functions"]},{"cell_type":"markdown","source":["## Weight initializer"],"metadata":{"id":"doBsNkg4jH4y"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"IaIkEMX6jqsZ","executionInfo":{"status":"ok","timestamp":1677306791368,"user_tz":-210,"elapsed":4,"user":{"displayName":"Amir Hamidi","userId":"14224589576575881828"}}},"outputs":[],"source":["def weight_initializer(module):\n","\n","    if isinstance(module, nn.Conv2d):\n","        nn.init.xavier_normal_(module.weight)\n","        nn.init.constant_(module.bias, 0.001)\n","\n","    elif isinstance(module, nn.Linear):\n","        nn.init.xavier_normal_(module.weight)\n","        nn.init.constant_(module.bias, 0.001)"]},{"cell_type":"markdown","source":["## Accuracy Computer"],"metadata":{"id":"bXq3nsG4jNqi"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"GP51-tkQ9gAr","executionInfo":{"status":"ok","timestamp":1677306791933,"user_tz":-210,"elapsed":3,"user":{"displayName":"Amir Hamidi","userId":"14224589576575881828"}}},"outputs":[],"source":["def Accuracy_Computer(Model, Dataset, Device):\n","\n","    Corrects = 0\n","    Samples = 0\n","    Model.eval()\n","\n","    with torch.no_grad():\n","\n","        for image, label in Dataset:\n","            \n","            # Transfer Data to GPU\n","            image = image.to(device=Device)\n","            label = label.to(device=Device)\n","\n","            # forward(prediction)\n","            label_prime = Model(image)\n","            _, predictions = label_prime.max(1)\n","            \n","            # determine the correct samples\n","            Corrects += (predictions == label).sum()\n","            Samples += label.shape[0]\n","\n","        Accuracy =  (Corrects / Samples) * 100\n","\n","    return float(Accuracy)"]},{"cell_type":"markdown","metadata":{"id":"aqH-C8mMuSQy"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOOYU_v-gvBV"},"outputs":[],"source":["# Validation_Type Options:\n","\n","# 1-\"KFold\"\n","# 2-\"Split\"\n","# 3-\"LOO\"\n","\n","Validation_Type = \"LOO\"\n","\n","if Validation_Type == \"KFold\":\n","\n","    Root_Folder = \"AllSubjects\"\n","\n","elif Validation_Type == \"Split\":\n","\n","    Root_Folder = \"AllSubjects\"\n","\n","elif Validation_Type == \"LOO\":\n","\n","    Root_Folder = \"SubjectWise\"\n","\n","else:\n","\n","    raise ValueError(\"no option is choosed\")\n","\n","if not os.path.isdir(f\"/content/{Root_Folder}\"):\n","\n","    if not os.path.isdir(\"/content/Drive\"):\n","\n","        drive.mount(\"Drive\", force_remount=True)\n","\n","    ZipFile = f'/content/Drive/MyDrive/Thesis/{Root_Folder}.zip'\n","    !unzip -qq {ZipFile}"]},{"cell_type":"markdown","metadata":{"id":"8btsdkUrN8O3"},"source":["# Hyper Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wNkva4wV1I_X"},"outputs":[],"source":["# Dataset Parameters\n","Batch_Size = 150\n","Train_Split = 0.9\n","Composit_Transformer = transforms.Compose([ToNumpy(), transforms.ToTensor()])\n","\n","# Training Parameters\n","Folds = 10\n","Epochs = 30\n","Learning_Rate = 1e-4\n","Device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"kxxoyQusN_Cp"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmcgGN-f3l5Z"},"outputs":[],"source":["if Validation_Type == \"KFold\":\n","\n","    Full_Dataset = Medical_Dataset(f\"/content/{Root_Folder}\", XTransformer=Composit_Transformer)\n","    image, _ = Full_Dataset.__getitem__(0)\n","\n","    if Folds > 1 :\n","\n","        kfold = KFold(n_splits=Folds, shuffle=True)\n","        Data_Fold = kfold.split(Full_Dataset)\n","\n","    else:\n","\n","        raise ValueError(\"Number of Folds should be greather than 1 when validation type is set to KFold\")\n","\n","    Train_Size = round((Folds - 1)/Folds * len(Full_Dataset))\n","    Test_Size = round(1/Folds * len(Full_Dataset))\n","\n","elif Validation_Type == \"Split\":\n","\n","    Folds = 1\n","    Full_Dataset = Medical_Dataset(f\"/content/{Root_Folder}\", XTransformer=Composit_Transformer)\n","    image, _ = Full_Dataset.__getitem__(0)\n","\n","    Train_Size = int(len(Full_Dataset) * Train_Split)\n","    Test_Size = len(Full_Dataset) - Train_Size\n","\n","    Train_Dataset, Test_Dataset = random_split(Full_Dataset, [Train_Size, Test_Size])\n","\n","elif Validation_Type == \"LOO\":\n","    \n","    Folds = 1\n","    Validation_Subjects = {\"HC\":[], \"PD\":[\"S_13\"]}\n","\n","    Train_Dataset = Medical_Dataset(f\"/content/{Root_Folder}\", XTransformer=Composit_Transformer, Validation_Type=Validation_Type, \n","                                    Train_Dataset=True, Validation_Subjects=Validation_Subjects)\n","    Test_Dataset = Medical_Dataset(f\"/content/{Root_Folder}\", XTransformer=Composit_Transformer, Validation_Type=Validation_Type, \n","                                   Train_Dataset=False, Validation_Subjects=Validation_Subjects)\n","    \n","    Train_Size = len(Train_Dataset)\n","    Test_Size = len(Test_Dataset)\n","\n","    image, _ = Train_Dataset.__getitem__(0)\n","\n","input_size = image.shape\n","\n","print(f\"Train Size: {Train_Size}\")\n","print(f\"Test Size: {Test_Size}\")"]},{"cell_type":"markdown","metadata":{"id":"_20FSEwY3Y3R"},"source":["# Defining and Initializing the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uy6Z9xds3Aw0"},"outputs":[],"source":["# Defining the Model\n","Model = CNN_Model(input_size, 2).to(Device)\n","\n","# Summary of the Model\n","Summary(Model, input_size=input_size)"]},{"cell_type":"markdown","source":["# Defining Loss Function and Optimizer"],"metadata":{"id":"t0AWqVxi4MSy"}},{"cell_type":"code","source":["# Defining Loss Function\n","Criteria = nn.CrossEntropyLoss()\n","\n","# Defining Optimizer\n","Optimizer = optim.Adam(Model.parameters(), lr=Learning_Rate, weight_decay=1e-2)"],"metadata":{"id":"micYiCus4Kti"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BM_EYI73eB9o"},"source":["# Training the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nn5fWudmHxTW"},"outputs":[],"source":["Total_Train_Accuracy = np.zeros((Epochs, Folds))\n","Total_Test_Accuracy = np.zeros((Epochs, Folds))\n","\n","Total_Train_Loss = np.zeros((Epochs, Folds))\n","Total_Test_Loss = np.zeros((Epochs, Folds))\n","\n","for fold in range(1, Folds + 1):\n","\n","    if Validation_Type == \"KFold\":\n","\n","        Train_indices, Test_indices = next(iter(Data_Fold))\n","\n","        Train_Sampler = SubsetRandomSampler(Train_indices)\n","        Test_Sampler = SubsetRandomSampler(Test_indices)\n","        Test_Size = len(Test_indices)\n","\n","        Train_Loader = DataLoader(dataset=Full_Dataset, batch_size=Batch_Size, sampler=Train_Sampler)\n","        Test_Loader = DataLoader(dataset=Full_Dataset, batch_size=Test_Size, sampler=Test_Sampler)\n","\n","        print(\"\\n\",\n","        f\"Fold {fold}/{Folds}\",\n","        \"\\n\",\n","        \"=\" * 25,\n","        \"\\n\", sep=\"\")\n","    \n","    elif Validation_Type == \"Split\":\n","\n","        Train_Loader = DataLoader(dataset=Train_Dataset, batch_size=Batch_Size, shuffle=True, pin_memory=True, num_workers=2)\n","        Test_Loader = DataLoader(dataset=Test_Dataset, batch_size=Test_Size, shuffle=True, pin_memory=True, num_workers=2)\n","\n","    elif Validation_Type == \"LOO\":\n","\n","        Train_Loader = DataLoader(dataset=Train_Dataset, batch_size=Batch_Size, shuffle=True, pin_memory=True, num_workers=2)\n","        Test_Loader = DataLoader(dataset=Test_Dataset, batch_size=Test_Size, shuffle=True, pin_memory=True, num_workers=2)\n","\n","    # Initializing the Model\n","    Model.apply(weight_initializer)\n","\n","    # Loop Through Epochs\n","    for epoch in range(1, Epochs + 1):\n","\n","            Start = time()\n","            Train_Loss = 0\n","            Test_Loss = 0\n","\n","            # Loop Through Bacthes\n","            for Batch_Number, (image, label) in enumerate(Train_Loader, 1):\n","                \n","                # Retrive the batch of image and labels from Dataset\n","                image = image.to(device=Device)\n","                label = label.to(device=Device)\n","                \n","                # zero the parameter gradient\n","                Optimizer.zero_grad()\n","\n","                # Forward Step\n","                output = Model(image)\n","                \n","                # Backward Step\n","                Loss = Criteria(output, label)\n","                Loss.backward()\n","\n","                # Opimization Step\n","                Optimizer.step()\n","\n","                # Calculating the Loss in an epoch\n","                Train_Loss += Loss.item()\n","            \n","            Train_Loss = Train_Loss / Batch_Number\n","\n","            # Calculating the Test Loss\n","            with torch.no_grad():\n","\n","                image , label = next(iter(Test_Loader))\n","\n","                # # Transfer data to Device\n","                label = label.to(Device)\n","                image = image.to(Device)\n","\n","                # Forward Propagation and Calculate the Loss\n","                output = Model(image)\n","                Loss = Criteria(output, label)\n","\n","                # Calculating loss for all the Test Dataset\n","                Test_Loss += Loss.item()\n","\n","            Train_Accuracy =  Accuracy_Computer(Model, Train_Loader, Device)\n","            Test_Accuracy = Accuracy_Computer(Model, Test_Loader, Device)\n","\n","            Total_Train_Loss[epoch - 1, fold - 1] = Train_Loss\n","            Total_Test_Loss[epoch - 1, fold - 1] = Test_Loss\n","\n","            Total_Train_Accuracy[epoch - 1, fold - 1] = Train_Accuracy\n","            Total_Test_Accuracy[epoch - 1, fold - 1] = Test_Accuracy\n","\n","            End = time()\n","            print(f\"Epoch {epoch}/{Epochs} : (Elapsed Time = {round(End-Start, 2)} Seconds)\\n {' ' * 5} \"\n","                  f\"[Train Loss = {Train_Loss:.5f}] - [Train Accuracy = {(Train_Accuracy):.2f}%] | \"\n","                  f\"[Test Loss = {Test_Loss:.5f}] - [Test Accuracy = {(Test_Accuracy):.2f}%] \\n \"\n","                  f\"{' ' * 4 + '-' * 105}\")"]},{"cell_type":"code","source":["for fold in range(1 , Folds + 1):\n","    \n","    if Validation_Type == \"KFold\":\n","\n","        print(\"\\n\",\n","              f\"Fold {fold}/{Folds}\",\n","              \"\\n\",\n","              \"=\" * 25,\n","              \"\\n\", sep=\"\")\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(Total_Train_Accuracy[:, fold - 1])\n","    plt.plot(Total_Test_Accuracy [:, fold - 1])\n","    plt.legend([\"Train Accuracy\", \"Test Accuracy\"])\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(Total_Train_Loss[:, fold - 1])\n","    plt.plot(Total_Test_Loss[:, fold - 1])\n","    plt.legend([\"Train Loss\", \"Test Loss\"])\n","\n","    plt.show()"],"metadata":{"id":"otpRIrpPyPTl"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["MaEHh974EVSI","e10-bYLj3GRh","VquRrPAuzRlP","cBnMwcfBi8ab","05P0FnxkjVaj","i7WaIed6T1jg","doBsNkg4jH4y","bXq3nsG4jNqi"],"authorship_tag":"ABX9TyNdTJE/Dj1nZXhEqX03lwjG"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}